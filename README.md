# NVIDIA GPU Operator ‡∏ö‡∏ô AWS Kubernetes Cluster - Complete Guide

## üèóÔ∏è Phase 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment (‡∏ó‡∏∏‡∏Å nodes)

### 1.1 ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡∏ó‡∏∏‡∏Å nodes (control plane + workers)
sudo apt update && sudo apt install -y apt-transport-https ca-certificates curl gpg
```

### 1.2 ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Container Runtime (containerd)
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡∏ó‡∏∏‡∏Å nodes
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
sudo systemctl restart containerd && sudo systemctl enable containerd
```

### 1.3 Configure Network ‡πÅ‡∏•‡∏∞ Kernel Modules
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡∏ó‡∏∏‡∏Å nodes
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay && sudo modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
sudo sysctl --system
```

---

## ‚öôÔ∏è Phase 2: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Kubernetes

### 2.1 ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á kubeadm, kubelet, kubectl
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡∏ó‡∏∏‡∏Å nodes
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update && sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```

### 2.2 Initialize Control Plane (‡∏£‡∏±‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Control Plane Node)
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô control plane node ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --control-plane-endpoint=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)

# Setup kubectl ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö user
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

### 2.3 ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á CNI Plugin (Flannel)
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô control plane node
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
```

### 2.4 Join Worker Nodes
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô control plane ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π join command
kubeadm token create --print-join-command

# ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å output ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏±‡∏ô‡∏ö‡∏ô worker nodes
# ‡∏ï‡∏≠‡∏ß‡πà‡∏≤‡∏á‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô: sudo kubeadm join <CONTROL_PLANE_IP>:6443 --token <TOKEN> --discovery-token-ca-cert-hash sha256:<HASH>
```

### 2.5 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Cluster Status
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô control plane
kubectl get nodes -o wide
kubectl get pods -n kube-system
```

---

## üöÄ Phase 3: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á NVIDIA GPU Operator

### 3.1 ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Helm
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô control plane
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
```

### 3.2 Add NVIDIA Helm Repository
```bash
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia && helm repo update
```

### 3.3 ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á GPU Operator
```bash
helm install --wait gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator-resources \
  --create-namespace \
  --set driver.enabled=true \
  --set toolkit.enabled=true \
  --set devicePlugin.enabled=true \
  --set nodeStatusExporter.enabled=true \
  --set gfd.enabled=true \
  --set migManager.enabled=false \
  --set operator.cleanupCRD=true
```

### 3.4 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á GPU Operator
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö pods
kubectl get pods -n gpu-operator-resources

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU nodes
kubectl get nodes -l nvidia.com/gpu.present=true

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU resources
kubectl describe nodes | grep -A 10 "Allocatable:" | grep nvidia.com/gpu
```
