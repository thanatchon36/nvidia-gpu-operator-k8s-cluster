# NVIDIA GPU Operator ‡∏ö‡∏ô AWS Kubernetes Cluster - Complete Guide

## üèóÔ∏è Phase 1: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Environment (‡∏ó‡∏∏‡∏Å nodes)

### 1.1 ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Dependencies
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡∏ó‡∏∏‡∏Å nodes (control plane + workers)
sudo apt update && sudo apt install -y apt-transport-https ca-certificates curl gpg
```

### 1.2 ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Container Runtime (containerd)
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡∏ó‡∏∏‡∏Å nodes
sudo apt install -y containerd
sudo mkdir -p /etc/containerd
containerd config default | sudo tee /etc/containerd/config.toml
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
sudo systemctl restart containerd && sudo systemctl enable containerd
```

### 1.3 Configure Network ‡πÅ‡∏•‡∏∞ Kernel Modules
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡∏ó‡∏∏‡∏Å nodes
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
sudo modprobe overlay && sudo modprobe br_netfilter

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
sudo sysctl --system
```

---

## ‚öôÔ∏è Phase 2: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Kubernetes

### 2.1 ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á kubeadm, kubelet, kubectl
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô‡∏ó‡∏∏‡∏Å nodes
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
sudo apt update && sudo apt install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl
```

### 2.2 Initialize Control Plane (‡∏£‡∏±‡∏ô‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Control Plane Node)
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô control plane node ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
sudo kubeadm init --pod-network-cidr=10.244.0.0/16 --control-plane-endpoint=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)

# Setup kubectl ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö user
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

### 2.3 ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á CNI Plugin (Flannel)
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô control plane node
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
```

### 2.4 Join Worker Nodes
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô control plane ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π join command
kubeadm token create --print-join-command

# ‡∏Ñ‡∏±‡∏î‡∏•‡∏≠‡∏Å output ‡πÅ‡∏•‡πâ‡∏ß‡∏£‡∏±‡∏ô‡∏ö‡∏ô worker nodes
# ‡∏ï‡∏≠‡∏ß‡πà‡∏≤‡∏á‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô: sudo kubeadm join <CONTROL_PLANE_IP>:6443 --token <TOKEN> --discovery-token-ca-cert-hash sha256:<HASH>
```

### 2.5 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Cluster Status
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô control plane
kubectl get nodes -o wide
kubectl get pods -n kube-system
```

---

## üöÄ Phase 3: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á NVIDIA GPU Operator

### 3.1 ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Helm
```bash
# ‡∏£‡∏±‡∏ô‡∏ö‡∏ô control plane
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
```

### 3.2 Add NVIDIA Helm Repository
```bash
helm repo add nvidia https://helm.ngc.nvidia.com/nvidia && helm repo update
```

### 3.3 ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á GPU Operator
```bash
helm install --wait gpu-operator nvidia/gpu-operator \
  --namespace gpu-operator-resources \
  --create-namespace \
  --set driver.enabled=true \
  --set toolkit.enabled=true \
  --set devicePlugin.enabled=true \
  --set nodeStatusExporter.enabled=true \
  --set gfd.enabled=true \
  --set migManager.enabled=false \
  --set operator.cleanupCRD=true
```

### 3.4 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á GPU Operator
```bash
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö pods
kubectl get pods -n gpu-operator-resources

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU nodes
kubectl get nodes -l nvidia.com/gpu.present=true

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö GPU resources
kubectl describe nodes | grep -A 10 "Allocatable:" | grep nvidia.com/gpu
```

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö NVIDIA Driver DaemonSet ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á GPU ‡∏ö‡∏ô Kubernetes

## 1. ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö DaemonSet ‡∏Ç‡∏≠‡∏á NVIDIA Driver

‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Pod ‡∏Ç‡∏≠‡∏á `nvidia-driver-daemonset` ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà:

```bash
kubectl get pods -A | grep 'nvidia-driver-daemonset'
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á):**

```
gpu-operator-resources   nvidia-driver-daemonset-mpgnt   2/2     Running   0          5m
gpu-operator-resources   nvidia-driver-daemonset-xk2p9   2/2     Running   0          5m
```

* **Namespace**: `gpu-operator-resources`
* **‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞**: `Running` (‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà ‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö log ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°)

---

## 2. ‡∏£‡∏±‡∏ô `nvidia-smi` ‡∏ú‡πà‡∏≤‡∏ô Container

‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤ NVIDIA driver ‡∏ñ‡∏π‡∏Å‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞ GPU ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ô‡∏µ‡πâ:

```bash
kubectl exec -n gpu-operator-resources -it nvidia-driver-daemonset-mpgnt -c nvidia-driver-ctr -- nvidia-smi
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á):**

```
Tue Aug 20 08:35:11 2024
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 535.54.03    Driver Version: 535.54.03    CUDA Version: 12.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|  0  Tesla T4            Off  | 00000000:00:1E.0 Off |                    0 |
| N/A   45C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |
+-----------------------------------------------------------------------------+
```

---

## 3. Troubleshooting

‡∏´‡∏≤‡∏Å‡∏û‡∏ö‡∏õ‡∏±‡∏ç‡∏´‡∏≤:

* ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î Pod:

  ```bash
  kubectl describe pod -n gpu-operator-resources <pod-name>
  ```
* ‡∏î‡∏π log ‡∏Ç‡∏≠‡∏á container:

  ```bash
  kubectl logs -n gpu-operator-resources <pod-name> -c nvidia-driver-ctr
  ```
* ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Node GPU worker ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πå‡∏î NVIDIA GPU ‡∏à‡∏£‡∏¥‡∏á
  (‡πÄ‡∏ä‡πà‡∏ô ‡∏ö‡∏ô AWS instance `g4dn.xlarge` ‡∏à‡∏∞‡∏°‡∏µ **NVIDIA T4**)

---

### 3.4 ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á GPU Operator
‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏™‡πà‡∏ß‡∏ô‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏Ç‡∏≠‡∏á GPU Operator ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á Pods ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô namespace ‡∏Ç‡∏≠‡∏á GPU Operator [1].

```bash
kubectl get pods -n gpu-operator-resources
```

**‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á (‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á):**
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏ß‡∏£‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô Pods ‡∏´‡∏•‡∏±‡∏Å‡πÜ ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ `Running` ‡∏´‡∏£‡∏∑‡∏≠ `Completed`

```NAME                                                          READY   STATUS      RESTARTS   AGE
gpu-feature-discovery-s876d                                   1/1     Running     0          10m
nvidia-container-toolkit-daemonset-hms7l                      1/1     Running     0          10m
nvidia-dcgm-exporter-sf74v                                    1/1     Running     0          10m
nvidia-device-plugin-daemonset-xzp6w                          1/1     Running     0          10m
nvidia-driver-daemonset-mpgnt                                 2/2     Running     0          10m
```

*   **`gpu-feature-discovery`**: ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏õ‡πâ‡∏≤‡∏¢ (label) ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö Node ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πå‡∏î‡∏à‡∏≠ GPU [1].
*   **`nvidia-container-toolkit-daemonset`**: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ Container Runtime (‡πÄ‡∏ä‡πà‡∏ô Docker, containerd) ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô GPU ‡πÑ‡∏î‡πâ [11].
*   **`nvidia-device-plugin-daemonset`**: ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏®‡∏à‡∏≥‡∏ô‡∏ß‡∏ô GPU ‡∏ó‡∏µ‡πà‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏ô Node ‡πÉ‡∏´‡πâ Kubernetes ‡∏ó‡∏£‡∏≤‡∏ö [1].
*   **`nvidia-driver-daemonset`**: ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á NVIDIA driver ‡∏ö‡∏ô Node [9].
*   **`nvidia-dcgm-exporter`**: (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ) ‡∏£‡∏ß‡∏ö‡∏£‡∏ß‡∏°‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å (metrics) ‡∏à‡∏≤‡∏Å GPU ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ Monitoring.

‡∏´‡∏≤‡∏Å Pod ‡πÉ‡∏î‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á `kubectl logs` ‡πÅ‡∏•‡∏∞ `kubectl describe pod` ‡πÉ‡∏ô Section 3 ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏ï‡πà‡∏≠‡πÑ‡∏õ

## 4. ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á

* [NVIDIA GPU Operator](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html)
* [NVIDIA Kubernetes Device Plugin](https://github.com/NVIDIA/k8s-device-plugin)
```
